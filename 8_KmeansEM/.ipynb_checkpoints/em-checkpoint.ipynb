{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6 0.4]\n",
      " [0.5 0.5]]\n",
      "Iteration: 5\n",
      "theta_A = 0.71, theta_B = 0.58, difference in loglike = -32.69\n",
      "Iteration: 5\n",
      "theta_A = 0.75, theta_B = 0.57, difference in loglike = 1.43\n",
      "Iteration: 5\n",
      "theta_A = 0.77, theta_B = 0.55, difference in loglike = 0.50\n",
      "Iteration: 5\n",
      "theta_A = 0.78, theta_B = 0.53, difference in loglike = 0.43\n",
      "Iteration: 5\n",
      "theta_A = 0.79, theta_B = 0.53, difference in loglike = 0.26\n",
      "Iteration: 5\n",
      "theta_A = 0.79, theta_B = 0.52, difference in loglike = 0.12\n",
      "Iteration: 5\n",
      "theta_A = 0.80, theta_B = 0.52, difference in loglike = 0.05\n",
      "Iteration: 5\n",
      "theta_A = 0.80, theta_B = 0.52, difference in loglike = 0.02\n",
      "Iteration: 5\n",
      "theta_A = 0.80, theta_B = 0.52, difference in loglike = 0.01\n"
     ]
    }
   ],
   "source": [
    "#EM algorithm implementation\n",
    "import numpy as np\n",
    "ys = np.array([(5,5), (9,1), (8,2), (4,6), (7,3)])\n",
    "#print(ys)\n",
    "thetas = np.array([[0.6, 0.4], [0.5, 0.5]])  # initialize theta_1,2\n",
    "#print(thetas)\n",
    "pis = np.array([0.5,0.5])  # prob of choosing coin 1 or coin2 is the same.  \n",
    "\n",
    "tolerance = 0.01\n",
    "max_iter = 100\n",
    "\n",
    "loglike_old = 0\n",
    "for i in range(max_iter):\n",
    "    E_c1 = []\n",
    "    E_c2 = []\n",
    "    EcY_1 = []\n",
    "    EcY_2 = []\n",
    "    loglike_new = 0\n",
    "    # E-step: calculate probability distributions over possible completions\n",
    "    for i in xrange(len(ys)):\n",
    "\n",
    "        # multinomial (binomial) log likelihood\n",
    "        log_k1 = np.sum([ys[i]*np.log(thetas[0])])  #  \\log [\\theta_k^{y_{oi}} (1-\\theta_k)^{n - y_{oi}} ]  \n",
    "        log_k2 = np.sum([ys[i]*np.log(thetas[1])])  #  \\log [\\theta_k^{y_{oi}} (1-\\theta_k)^{n - y_{oi}} ] \n",
    "\n",
    "        # Getting the expectation of c_ik\n",
    "        denom = np.exp(log_k1) * pis[0] + np.exp(log_k2) * pis[1]\n",
    "        E_ci1 = np.exp(log_k1) * pis[0] / denom\n",
    "        E_ci2 = np.exp(log_k2) * pis[1] / denom\n",
    "\n",
    "        # update complete log likelihood  \n",
    "        # we it only to check if it converged. \n",
    "        # we dont need it for updating theta. \n",
    "        loglike_new += E_ci1 * log_k1 + E_ci2 * log_k2\n",
    "        E_c1.append(E_ci1)\n",
    "        E_c2.append(E_ci2)\n",
    "\n",
    "    # M-step: update values for parameters given current distribution\n",
    "    for i in xrange(len(ys)):\n",
    "        EcY_1.append(E_c1[i] * ys[i] )  # this is a scalar times a vector.\n",
    "        EcY_2.append(E_c2[i] * ys[i] )\n",
    "    thetas[0] = np.sum(EcY_1, 0)/np.sum(EcY_1)\n",
    "    thetas[1] = np.sum(EcY_2, 0)/np.sum(EcY_2)\n",
    "    print( \"Iteration: %d\" % (i+1))\n",
    "    print(\"theta_A = %.2f, theta_B = %.2f, difference in loglike = %.2f\" % (thetas[0,0], thetas[1,0], loglike_new - loglike_old))\n",
    "\n",
    "    if np.abs(loglike_new - loglike_old) < tolerance:\n",
    "        break\n",
    "    loglike_old = loglike_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
